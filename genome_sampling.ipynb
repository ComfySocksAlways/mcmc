{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hopsy\n",
    "from PolyRound.api import PolyRoundApi\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restricted license - for non-production use only - expires 2025-11-24\n"
     ]
    }
   ],
   "source": [
    "model_path = os.path.join(\"hopsy\",\"examples\",\"test_data\", \"e_coli_core.xml\")\n",
    "polytope = PolyRoundApi.sbml_to_polytope(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing rounding transformation took 2.1488025359999483 seconds\n"
     ]
    }
   ],
   "source": [
    "problem = hopsy.Problem(polytope.A, polytope.b)\n",
    "problem = hopsy.add_box_constraints(problem, upper_bound=10_000, lower_bound=-10_000, simplify=True)\n",
    "start = time.perf_counter()\n",
    "problem = hopsy.round(problem)\n",
    "print(\"Computing rounding transformation took\", time.perf_counter()-start,\"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(190, 95)\n",
      "(190,)\n",
      "(190, 95)\n",
      "(190,)\n"
     ]
    }
   ],
   "source": [
    "print(polytope.A.shape)\n",
    "print(polytope.b.shape)\n",
    "print(problem.A.shape)\n",
    "print(problem.b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 511\n",
    "chains, rngs = hopsy.setup(problem, seed, n_chains=4)\n",
    "n_samples = 100_000\n",
    "# Either use thinning rule, see  10.1371/journal.pcbi.1011378\n",
    "# or use one-shot transformation (for expert users). We show one-shot transformation at the end.\n",
    "thinning = int(1./6*problem.transformation.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Sampling with Transform <h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'False'\n"
     ]
    }
   ],
   "source": [
    "%%script False\n",
    "start = time.perf_counter()\n",
    "accrate, samples = hopsy.sample(chains, rngs, n_samples, thinning=thinning, n_procs=4)\n",
    "# accrate is 1 for uniform samples with the default chains given by hopsy.setup()\n",
    "print(\"sampling with internal trafo took\", time.perf_counter()-start,\"seconds\")\n",
    "print(samples.shape)\n",
    "rhat = np.max(hopsy.rhat(samples))\n",
    "print(\"rhat:\", rhat)\n",
    "ess = np.min(hopsy.ess(samples)) / len(chains)\n",
    "print(\"ess:\", ess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Sampling without Transform<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling took 1.349342826000111 seconds\n"
     ]
    }
   ],
   "source": [
    "assert problem.transformation is not None\n",
    "# deep copy enures that we do not edit the original problem\n",
    "problem2 = copy.deepcopy(problem)\n",
    "problem2.transformation=None\n",
    "problem2.shift=None\n",
    "seed = 512\n",
    "chains, rngs = hopsy.setup(problem2, seed, n_chains=4)\n",
    "# thinning is still advised when hard drive memory is limisted to not to store too many samples \n",
    "thinning = int(1./6*problem.A.shape[1])  \n",
    "\n",
    "start = time.perf_counter()\n",
    "accrate, sample_stack = hopsy.sample(chains, rngs, n_samples, thinning=thinning, n_procs=4)\n",
    "# accrate is 1 for uniform samples with the default chains given by hopsy.setup()\n",
    "print(\"sampling took\", time.perf_counter()-start,\"seconds\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'False'\n"
     ]
    }
   ],
   "source": [
    "%%script False\n",
    "print('sample shape', sample_stack.shape)\n",
    "rhat = np.max(hopsy.rhat(sample_stack))\n",
    "print(\"rhat:\", rhat)\n",
    "ess = np.min(hopsy.ess(sample_stack)) / len(chains)\n",
    "print(\"ess:\", ess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make copy of sample_stack\n",
    "sample_stack_seq = sample_stack.copy()\n",
    "sample_stack_pl = sample_stack.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n"
     ]
    }
   ],
   "source": [
    "print(sample_stack.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Transform Back: Sequential<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformation took 0.46120086100017943 seconds\n",
      "sample stats are the same (save numerics) before and after the linear transformation:\n"
     ]
    }
   ],
   "source": [
    "# transform samples back all at once\n",
    "shift_t = np.array([problem.shift]).T\n",
    "start_trafo = time.perf_counter()\n",
    "full_samples = np.zeros((len(chains), n_samples, sample_stack.shape[2]))\n",
    "for i in range(len(chains)):\n",
    "    full_samples[i] = (problem.transformation@sample_stack[i].T).T + np.tile(shift_t, (1, n_samples)).T\n",
    "    \n",
    "print(\"transformation took\", time.perf_counter()-start_trafo,\"seconds\")\n",
    "print('sample stats are the same (save numerics) before and after the linear transformation:')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'False'\n"
     ]
    }
   ],
   "source": [
    "%%script False\n",
    "rhat = np.max(hopsy.rhat(full_samples))\n",
    "print(\"rhat:\", rhat)\n",
    "ess = np.min(hopsy.ess(full_samples)) / len(chains)\n",
    "print(\"ess:\", ess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Transform Back: Parallel<h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "from numba import *\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "# Device Properties to get execution config\n",
    "# Get the current device\n",
    "device = cuda.get_current_device()\n",
    "\n",
    "# Access device properties\n",
    "warp_size = device.WARP_SIZE\n",
    "multi_processor_count = device.MULTIPROCESSOR_COUNT\n",
    "\n",
    "# print(warp_size)\n",
    "# print(multi_processor_count)\n",
    "tpb = warp_size                     # threads per block\n",
    "nb = multi_processor_count * 32     # number of blocks\n",
    "print(tpb)\n",
    "print(nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This version needs A and b to be input as transposed.\n",
    "\"\"\"\n",
    "Perform Y = X.A + b\n",
    "X : (n_samples, A.shape[0]), n_samples is parallelised\n",
    "Y : same shape as X\n",
    "b : Row vector\n",
    "\"\"\"\n",
    "@cuda.jit\n",
    "def tranformv1(A:np.ndarray, b: np.ndarray, X: np.ndarray, Y: np.ndarray):\n",
    "    idx = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    for i in range(idx,X.shape[0],stride):\n",
    "        for j in range(A.shape[1]):\n",
    "            temp = 0\n",
    "            for k in range(A.shape[0]):\n",
    "                temp += X[i,k] * A[k,j]\n",
    "            Y[i,j] = temp + b[0,j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpb2d = tpb\n",
    "@cuda.jit\n",
    "def transformv2(A:np.ndarray, b: np.ndarray, X: np.ndarray, Y: np.ndarray):\n",
    "    # tpb2d, _ = cuda.blockDim(2)\n",
    "\n",
    "    sX = cuda.shared.array(shape=(tpb2d,tpb2d), dtype=float64)\n",
    "    sA = cuda.shared.array(shape=(tpb2d,tpb2d), dtype=float64)\n",
    "\n",
    "    x, y = cuda.grid(2)\n",
    "    # stridex, stridey = cuda.gridsize(2)\n",
    "\n",
    "    tx, ty = cuda.threadIdx.x, cuda.threadIdx.y \n",
    "\n",
    "    bpg = cuda.gridDim.x    # blocks per grid, aka grid Dim\n",
    "\n",
    "    \n",
    "    # for i in range(idx,Y.shape[0],stridex):\n",
    "    #     for j in range(idy, Y.shape[1],stridey):\n",
    "    temp = float64(0.)\n",
    "    for i in range(bpg):\n",
    "        \n",
    "        # Preload chunks of data into shared memory\n",
    "        sX[tx,ty] = 0 \n",
    "        sA[tx,ty] = 0\n",
    "        if y < X.shape[0] and (tx + i * tpb2d) < X.shape[1]:\n",
    "            sX[ty,tx] = X[y,tx + i * tpb2d]\n",
    "        if x < A.shape[1] and (ty + i * tpb2d) < A.shape[0]:\n",
    "            sA[ty, tx] = A[ty + i * tpb2d, x]\n",
    "\n",
    "        # wait till loading complete\n",
    "        cuda.syncthreads()\n",
    "\n",
    "        # Do partial row * col\n",
    "        for j in range(tpb2d):\n",
    "            temp += sX[ty, j] * sA[j, tx]\n",
    "\n",
    "        # Sync again\n",
    "        cuda.syncthreads()\n",
    "\n",
    "    # Put result back in\n",
    "    if y < Y.shape[0] and x < Y.shape[1]:\n",
    "        Y[y,x] = temp + b[0,x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'False'\n"
     ]
    }
   ],
   "source": [
    "%%script False\n",
    "shift_t_pl = np.array([problem.shift])\n",
    "start_trafo = time.perf_counter()\n",
    "full_samples_pl = np.zeros((len(chains), n_samples, sample_stack.shape[2]))\n",
    "\n",
    "# Device arrays\n",
    "d_transformation = cuda.to_device(problem.transformation.T)\n",
    "d_sample_stack = cuda.to_device(sample_stack)\n",
    "d_shift = cuda.to_device(shift_t_pl)\n",
    "d_full_samples = cuda.to_device(full_samples_pl)\n",
    "for i in range(len(chains)):\n",
    "    tranformv1[nb,tpb](d_transformation,\n",
    "               d_shift,\n",
    "               d_sample_stack[i],\n",
    "               d_full_samples[i])\n",
    "full_samples_pl = d_full_samples.copy_to_host()\n",
    "print(\"transformation took\", time.perf_counter()-start_trafo,\"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harris/miniconda3/envs/vmcmc/lib/python3.10/site-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: Host array used in CUDA kernel will incur copy overhead to/from device.\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformation took 0.7409150810017309 seconds\n"
     ]
    }
   ],
   "source": [
    "shift_t_pl = np.array([problem.shift])\n",
    "start_trafo = time.perf_counter()\n",
    "full_samples_pl = np.zeros((len(chains), n_samples, sample_stack.shape[2]))\n",
    "\n",
    "# Device arrays\n",
    "d_transformation = cuda.to_device(problem.transformation.T)\n",
    "d_sample_stack = cuda.to_device(sample_stack)\n",
    "d_shift = cuda.to_device(shift_t_pl)\n",
    "d_full_samples = cuda.to_device(full_samples_pl)\n",
    "streams = [cuda.stream() for _ in range(len(chains))]\n",
    "for i in range(len(chains)):\n",
    "    tranformv1[nb,tpb,streams[i]](d_transformation,\n",
    "               d_shift,\n",
    "               d_sample_stack[i],\n",
    "               d_full_samples[i])\n",
    "full_samples_pl = d_full_samples.copy_to_host()\n",
    "print(\"transformation took\", time.perf_counter()-start_trafo,\"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script False\n",
    "rhat = np.max(hopsy.rhat(full_samples_pl))\n",
    "print(\"rhat:\", rhat)\n",
    "ess = np.min(hopsy.ess(full_samples_pl)) / len(chains)\n",
    "print(\"ess:\", ess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error  = 0\n",
    "for i in range(len(chains)):\n",
    "    error += np.linalg.norm(full_samples[i] - full_samples_pl[i],2)\n",
    "\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Setup\n",
    "N = 20\n",
    "A = np.random.random((5,5)).astype(np.float32)\n",
    "X = np.random.random((N,5)).astype(np.float32)\n",
    "b = np.random.random((1,5)).astype(np.float32)\n",
    "Y = np.zeros_like(X).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32)\n",
      "(1, 1)\n"
     ]
    }
   ],
   "source": [
    "tpb2d_ = (tpb,tpb)\n",
    "# nb2d_ = (math.ceil(Y.shape[0]/tpb)(Y.s),math.cel)\n",
    "nb2d_ = (math.ceil(Y.shape[0] / tpb), math.ceil(Y.shape[1]/tpb))\n",
    "\n",
    "print(tpb2d_)\n",
    "print(nb2d_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth = X@A + np.tile(b,(N,1))\n",
    "btruth = X@A + b\n",
    "\n",
    "# tranformsl(A,b,X,Y)\n",
    "dA = cuda.to_device(A)\n",
    "dX = cuda.to_device(X)\n",
    "db = cuda.to_device(b)\n",
    "dY = cuda.to_device(Y)\n",
    "tranformv1[nb,tpb](dA,db,dX,dY)\n",
    "# transformv2[nb2d_,tpb2d_](dA,db,dX,dY)\n",
    "Y = dY.copy_to_host()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
